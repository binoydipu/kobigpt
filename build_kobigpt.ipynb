{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9256b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/tagore_poems.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ba6c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 2373387\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the dataset: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a7f01b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "বজাও রে মোহন বাঁশি।\n",
      "সারা দিবসক\n",
      "বিরহদহনদুখ,\n",
      "মরমক তিয়াষ নাশি।\n",
      "রিঝমনভেদন\n",
      "বাঁশরিবাদন\n",
      "কঁহা শিখলি রে কান?\n",
      "হানে থিরথির\n",
      "মরমঅবশকর\n",
      "লহু লহু মধুময় বাণ।\n",
      "ধসধস করতহ\n",
      "উরহ বিয়াকুলু,\n",
      "ঢুলু ঢুলু অবশনয়ান ;\n",
      "কত কত বরষক\n",
      "বাত সোঁয়ারয়,\n",
      "অধীর করয় পরান।\n",
      "কত শত আশা\n",
      "পূরল না বঁধু,\n",
      "কত সুখ করল পয়ান।\n",
      "পহু গো কত শত\n",
      "পীরিতযাতন\n",
      "হিয়ে বিঁধাওল \n"
     ]
    }
   ],
   "source": [
    "print(text[:300]) # first 300 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "332e3253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),-.12679:;?BCFHJLMNORTW[]_abcdefghiklmnoprstuvwxy|ű̶।॥ঁংঃঅআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহািীুূৃেৈোৌ্ৎড়ঢ়য়০১২৩৪৫৬৭৮৯‍–—‘’“”\n",
      "\n",
      "Vocabulary Size: 139\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) # unique chars\n",
    "vocab_size = len(chars)         # no of unique chars\n",
    "print(''.join(chars))\n",
    "print(f'\\nVocabulary Size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad184263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers (0 - 138)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] if c in stoi else stoi[' '] for c in s] # encoder: take a string, output a list of integers (space for unknown chars)\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c1214df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 82, 107, 73, 1, 101, 113, 1, 99, 115, 106, 94, 1, 97, 107, 61, 103, 108, 59]\n",
      "বজাও রে মোহন বাঁশি।\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "enc_ = encode(\"বজাও রে মোহন বাঁশি।\")\n",
    "dec_ = decode(enc_)\n",
    "\n",
    "print(enc_)\n",
    "print(dec_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fddce17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters: []\n"
     ]
    }
   ],
   "source": [
    "prompt = \"বিনয় ভাবছে আকাশ কেন নীল,\"\n",
    "unknown_chars = [c for c in prompt if c not in stoi]\n",
    "print(\"Unknown characters:\", unknown_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b92fc7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 108, 94, 121, 1, 98, 107, 97, 81, 113, 1, 65, 75, 107, 103, 1, 75, 113, 94, 1, 94, 109, 102, 7]\n",
      "বিনয় ভাবছে আকাশ কেন নীল,\n"
     ]
    }
   ],
   "source": [
    "enc_ = encode(prompt)\n",
    "dec_ = decode(enc_)\n",
    "\n",
    "print(enc_)\n",
    "print(dec_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9439e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the entire text dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1dd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# (4, 8) tensor contains total 32 independent training examples\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f614ae",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "Only uses one token to predict the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add506d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # idx is token no, it takes out idx-th row from the table (C)\n",
    "        # it contains logits of all other tokens occurs after idx-th token\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) -> (batch_size, block_size, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # every B*T contains a token for which we want to run prediction \n",
    "            logits = logits.view(B*T, C) # 2-dim \n",
    "            targets = targets.view(B*T)  # 1-dim\n",
    "            \n",
    "            # cross_entropy need (B,C,T) Tensor\n",
    "            loss = F.cross_entropy(logits, targets) # calculate error of the prediction\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # for all B (batch dims), generate tokens for T (time) dims\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            # only last item of T (time) dim predicts what comes next\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax across C (total token) dim to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution, for each batch we predict 1 token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "res = bigram.generate(idx = torch.zeros((2, 1), dtype=torch.long), max_new_tokens=100)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum possible cross-entropy loss given vocab_size\n",
    "# note: if the model assigns zero probability to the true class the loss -> +inf.\n",
    "# a useful finite reference is the loss for a uniform prediction:\n",
    "#   -log(1/vocab_size) = log(vocab_size) (natural log)\n",
    "max_loss_ln = torch.log(torch.tensor(vocab_size, dtype=torch.float32)).item()\n",
    "\n",
    "print(\"Theoretical maximum (unbounded): +inf  (if p_true -> 0)\")\n",
    "print(f\"Uniform-prediction cross-entropy = ln(vocab_size) = {max_loss_ln:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11430a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(res[1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f3d89",
   "metadata": {},
   "source": [
    "### Training Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24aa736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch Adam optimizer\n",
    "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in tqdm(range(10000)):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = bigram(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # make all grad from prev step to 0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275be7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(bigram.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b3b94",
   "metadata": {},
   "source": [
    "#### Math Tricks for Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d922a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for masked-self attention\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e18889",
   "metadata": {},
   "outputs": [],
   "source": [
    "a /= a.sum(1, keepdim=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2600dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = wei @ x  # batched matrix multiply\n",
    "# (T, T) @ (B, T, C)\n",
    "# (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bbf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triangular ones\n",
    "wei = torch.zeros((T, T))            # all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # where tril is 0, fill -inf\n",
    "wei = F.softmax(wei, dim=-1)   # softmax along the last dim (rows sum to 1)\n",
    "xbow2 = wei @ x                # batched matrix multiply\n",
    "xbow2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(xbow, xbow2) # Same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c5cf3",
   "metadata": {},
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single head of self-attention\n",
    "# Brief explanation:\n",
    "# for each token in the input, we want to compute a weighted average of \n",
    "#   the values of all the tokens that came before it (including itself)\n",
    "# the weights are determined by the similarity between the query of the current token and the keys of\n",
    "# all the tokens that came before it (including itself)\n",
    "# the queries, keys, and values are all linear projections of the input tokens\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triangular ones\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # where tril is 0, fill -inf\n",
    "wei = F.softmax(wei, dim=-1)   # softmax along the last dim (rows sum to 1)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5011137",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e119cb",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "torch.manual_seed(1337)\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256    # context length\n",
    "    vocab_size: int = None        # the size of the vocabulary\n",
    "    n_layer: int = 6        # number of layers\n",
    "    n_head: int = 6         # number of attention heads\n",
    "    n_embd: int = 384       # token embedding dimension\n",
    "    dropout: float = 0.0    # dropout rate\n",
    "    bias: bool = True       # use bias in the Linear & Norm layers\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "\n",
    "        self.is_causal = is_causal  # if True, apply causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.key = nn.Linear(config.n_embd, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(config.n_embd, self.head_size, bias=False)\n",
    "        self.value = nn.Linear(config.n_embd, self.head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # dk**-0.5 is scaled dot-product attention, helps with stability\n",
    "        dk = k.size(-1)\n",
    "        att = q @ k.transpose(-2, -1) * dk**-0.5 # (B,T,T)\n",
    "        if self.is_causal:\n",
    "            att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        att = F.softmax(att, dim=-1) # (B,T,T)\n",
    "        \n",
    "        att = self.dropout(att)\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "\n",
    "        out = att @ v # (B,T,head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_causal=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(config, is_causal) for _ in range(config.n_head)])\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate the output of all heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenate the output of all heads\n",
    "        \n",
    "        # project helps to mix the information from different heads\n",
    "        out = self.proj(out) # project back to the original embedding dimension\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # Gives model capacity to represent richer nonlinear interactions per token\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(), # Gaussian Error Linear Unit activation function\n",
    "            \n",
    "            # Ensures the output has the same shape as the input so it can be added back residually\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_causal=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(config, is_causal)\n",
    "        self.ffwd = FeedForward(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add residual connections around the two sub-layers\n",
    "        x = x + self.attn(self.ln_1(x))     # apply layer norm before self-attention\n",
    "        x = x + self.ffwd(self.ln_2(x))     # apply layer norm before feed-forward\n",
    "        return x\n",
    "\n",
    "\n",
    "class KobiGPTModel(nn.Module):\n",
    "    \"\"\" the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config, is_causal=True) for _ in range(config.n_layer)],\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights) \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # ----------- Input Embedding + Positional Encoding -----------\n",
    "\n",
    "        # for each index in idx, get the corresponding token embedding\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        \n",
    "        # positional embeddings for each position in the sequence\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,n_embd)\n",
    "        \n",
    "        # now add the two embeddings together to get the final token representation\n",
    "        # x now has both token identity and positional information\n",
    "        x = token_embd + pos_embd # (B,T,n_embd)\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # ----------- Forward the input through the Transformer -----------\n",
    "\n",
    "        # pass the input through the series of Transformer blocks \n",
    "        # each block contains self-attention and feed-forward layers\n",
    "        # final x has contextualized token representations\n",
    "        x = self.blocks(x) # (B,T,n_embd)\n",
    "        \n",
    "        # final layer norm to stabilize and normalize the output\n",
    "        x = self.ln_f(x) # (B,T,n_embd)\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # ----------- Output of the Language Model ---------------------\n",
    "        \n",
    "        # project the final hidden states to the vocabulary size to get logits for each token\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "        if targets is None: # during inference, we only have idx and no targets\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # every B*T contains a token for which we want to run prediction \n",
    "            logits = logits.view(B*T, C) # 2-dim \n",
    "            targets = targets.view(B*T)  # 1-dim\n",
    "            \n",
    "            # cross_entropy need (B,C,T) Tensor\n",
    "            loss = F.cross_entropy(logits, targets) # calculate error of the prediction\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temparature=1.0):\n",
    "        \"\"\"\n",
    "        Generate new tokens given a context idx. Tweak the temperature to control randomness.\n",
    "\n",
    "        Args:\n",
    "            idx: (B, T) array of indices in the current context\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: float value to modulate the next token probabilities\n",
    "        Returns:\n",
    "            idx: (B, T + max_new_tokens) array of indices in the extended context\n",
    "        Example:\n",
    "            >>> context = torch.zeros((1, 1), dtype=torch.long) # starting token\n",
    "            >>> generated = model.generate(context, max_new_tokens=100)\n",
    "            >>> print(decode(generated[0].tolist()))\n",
    "        \"\"\"\n",
    "\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # for all B (batch dims), generate tokens for T (time) dims\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.config.block_size:] # (B, block_size)\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            # only last item of T (time) dim predicts what comes next\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax across C (total token) dim to get probabilities\n",
    "            probs = F.softmax(logits / temparature, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution, for each batch we predict 1 token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32                                         # no of independent sequences processed in parallel\n",
    "max_iters = 5000                                       # no of steps to train\n",
    "eval_interval = 500                                     # interval to evaluate the loss\n",
    "learning_rate = 3e-4                                    # learning rate\n",
    "eval_iters = 100  \n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.2,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "kobigpt = KobiGPTModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dc184",
   "metadata": {},
   "source": [
    "### Training KobiGPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(kobigpt.parameters(), lr=learning_rate)\n",
    "print(sum(p.numel() for p in kobigpt.parameters())/1e6, 'M parameters')\n",
    "\n",
    "for step in tqdm(range(max_iters)):\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss(kobigpt)\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data)\n",
    "     \n",
    "    # evaluate the loss\n",
    "    logits, loss = kobigpt(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d307cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 100\n",
    "print(decode(kobigpt.generate(\n",
    "  idx = torch.zeros((1, 1), dtype=torch.long), \n",
    "  max_new_tokens=max_new_tokens)[0].tolist()\n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute min and max cross-entropy loss (natural log base, nats)\n",
    "min_loss = 0.0  # achievable when model assigns probability 1 to the true class\n",
    "max_loss_theoretical = float('inf')  # unbounded if p_true -> 0\n",
    "uniform_loss_ln = torch.log(torch.tensor(vocab_size, dtype=torch.float32)).item()  # -log(1/vocab_size)\n",
    "\n",
    "print(f\"Minimum cross-entropy (nats): {min_loss}\")\n",
    "print(\"Maximum cross-entropy (theoretical): +inf (if model assigns zero probability to the true class)\")\n",
    "print(f\"Reference (uniform prediction): ln(vocab_size) = {uniform_loss_ln:.6f} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56689bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation & guidance for generalization (use existing variables/functions)\n",
    "\n",
    "\n",
    "# Simple interpretation heuristics (char-level, nats):\n",
    "print(f\"\\nUniform (random) baseline loss = ln(vocab_size) = {uniform_loss_ln:.4f} nats\")\n",
    "print(\"Heuristics:\")\n",
    "print(\"- val_loss > uniform: model is worse than random baseline -> check bugs/underfitting or data issues\")\n",
    "print(\"- val_loss ≈ uniform but train_loss << val_loss: severe overfitting\")\n",
    "print(\"- reasonable targets (depend on data/model):\")\n",
    "print(\"    * val_loss < uniform (better than random) is the first milestone\")\n",
    "print(\"    * val_loss ~ 3-4 nats (ppl ~ 20-55) = modest language modelling quality\")\n",
    "print(\"    * val_loss < 2 nats (ppl < ~7) = very good for small-char models (hard to achieve)\")\n",
    "\n",
    "print(\"\\nPractical tips to improve generalization (short):\")\n",
    "print(\"- Reduce overfitting: increase dropout, weight decay, or early stopping; reduce model size\")\n",
    "print(\"- Reduce underfitting: train longer, increase capacity, or reduce regularization\")\n",
    "print(\"- More / cleaner data helps most. Monitor train vs val loss and generated samples.\")\n",
    "print(\"- Use validation loss trend + qualitative samples rather than aiming for absolute 0 (impossible).\")\n",
    "\n",
    "# Optional: show a short generated sample from the transformer to inspect quality\n",
    "print(\"\\nExample sample (KobiGPT):\")\n",
    "print(decode(kobigpt.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
