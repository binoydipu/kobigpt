# ğŸ­ KobiGPT â€” Rabindranath Tagore Poem Generator

KobiGPT is a **character-level GPT language model** trained exclusively on the complete collection of **Rabindranath Tagoreâ€™s Bengali poems**.  
It generates new verses that stylistically and linguistically resemble Tagoreâ€™s original poetic rhythm, word patterns, and emotional tone.

---

## ğŸ§  Model Overview

KobiGPT is based on a **GPT (Generative Pretrained Transformer)** architecture â€” a type of **decoder-only transformer** that predicts the next character in a sequence given the previous ones. Autoregressive modeling allows it to generate coherent text one character at a time.

- **Type:** Character-level GPT  
- **Architecture:** Decoder-only Transformer (like GPT-2 but smaller)  
- **Framework:** PyTorch  
- **Dataset:** Complete collection of Rabindranath Tagoreâ€™s Bengali poems  
- **Tokenizer:** Character-level (139 unique characters)  
- **Training Device:** NVIDIA T4 GPU (Google Colab Free Tier)

---

## âš™ï¸ Model Configuration

| **Parameter** | **Description / Value** |
|----------------|--------------------------|
| **Context Length (`block_size`)** | 256 characters |
| **Vocabulary Size (`vocab_size`)** | 139 unique characters |
| **Transformer Layers (`n_layer`)** | 6 |
| **Attention Heads (`n_head`)** | 8 |
| **Embedding Dimension (`n_embd`)** | 512 |
| **Dropout** | 0.2 |
| **Use Bias** | âœ… True |
| **Total Parameters** | â‰ˆ **19.18 million** |
| **Optimizer** | AdamW |
| **Initial Learning Rate** | 3 Ã— 10â»â´ |
| **LR Schedule** | Decayed by Ã—0.1 at steps **3000** and **5000** |
| **Batch Size** | 32 |
| **Training Steps** | 8000 |
| **Training Duration** | ~1.5 hours |
| **Hardware** | NVIDIA T4 GPU (Google Colab Free Tier) |


---

## ğŸ” Model Architecture Explained

### ğŸ”¸ Embedding Layer
Each character is mapped to a **512-dimensional vector** via a learnable embedding matrix.  
Positional embeddings are added to retain sequence order.

### ğŸ”¸ Transformer Blocks
The model consists of **6 layers**, each with:
- Masked multi-head self-attention (8 heads)
- Feed-forward projection (2-layer MLP)
- Layer normalization and residual connections

### ğŸ”¸ Output Projection
The final layer projects the hidden representation back into the vocabulary space, producing logits for each character.


---

## ğŸ§© Forward Pass (Intuitive Overview)

1. Input sequence (e.g., `"\n"`) is encoded into character IDs.  
2. Each ID is converted into embeddings and combined with positional encodings.  
3. The transformer layers use **self-attention** to model dependencies between all characters.  
4. The model predicts the next character probability distribution.  
5. Loss is computed via **cross-entropy**.



---

## ğŸ“ˆ Training Curve (Loss Progression)

| Metric | Value |
|---------|--------|
| Final Train Loss | 1.50 |
| Final Val Loss | 1.69 |
| Training Steps | 8000 |
| GPU Time | ~1.5 hours |

<p align="center">
  <img src="./output/logs/kobigpt_model_2_loss_graph.png" alt="Training and Validation Loss Curves" width="600"/>
</p>

The final validation loss of **~1.69 nats (â‰ˆ2.44 bits per character)** indicates strong character-level modeling capability for a small GPT trained on poetic text.

---

## ğŸª¶ Sample Generated Poem

From a generation run without any prompt (starting from `"\n"`):
```
à¦¹à¦¾à¦¸à¦¿ à¦›à§à¦Ÿà¦¿ à¦¤à¦¾à§‡ à¦—à¦¾à¦¨ à¦—à§‡à§Ÿà§‡,
à¦¤à¦¾à¦°à§‡ à¦à¦à¦•à§‡ à¦¦à§‡à¦¬à§‡ à¦¹à§‡à¦¸à§‡à¥¤
à¦ à¦­à§à¦¬à¦¨à¦šà¦°à¦£ à¦¤à§ƒà¦·à¦¾ à¦à¦¸à§‡ à¦¬à§‡à¦¦à¦¨à¦¾,
à¦¤à¦°à§à¦£ à¦¹à§ƒà¦¦à§Ÿ à¦‰à¦ à§‡ à¦¶à§à¦¯à¦¾à¦®à¦¾ à¦®à§‡à¦˜à§‡à¦° à¦«à§à¦²
à¦“à¦‡ à¦¯à§‡à¦¨ à¦†à¦²à§‹
à¦à¦¾à¦‰à§Ÿà§‡à¦°'à¦ªà¦°à§‡ à¦ªà§œà§‡ à¦†à¦œà¥¤
à¦†à¦•à¦¾à¦¶ à¦›à¦¿à¦à§œà§‡ à¦¨à¦¾à¦‡à¦² à¦šà§‹à¦–à§‡à¥¤
à¦¸à§à¦¨à§‡à¦¹à¦®à§à¦–à§€à¦° à¦›à§œà¦¾à§Ÿà§‡ à¦¨à¦¾à¦‡, à¦šà¦®à¦•à¦¿
à¦šà§‚à¦°à§à¦£ à¦›à§œà¦¿à§Ÿà¦¾ à¦šà¦²à§‡ à¦¯à¦¾à§Ÿ,
à¦¤à¦¾à¦° à¦•à¦ªà§‹à¦²à§‡ à¦¤à¦¾à¦° à¦¬à¦¾à¦²à§‡ à¦¦à§‡à¦–à§€,
à¦¬à¦¾à¦à¦§à¦¨ à¦•à§‡à¦à¦¦à§‡ à¦‰à§œà§‡ à¦¯à¦¾à§Ÿà¥¤
à¦¹à§ƒà¦¦à§Ÿ à¦•à§à¦¸à§à¦® à¦¸à¦¾à¦—à¦°à§‡
à¦®à¦¿à¦¶à§‡ à¦•à§‡ à¦œà¦¾à¦¨à¦¿à¦² à¦¨à¦¾ à¦•à¦–à¦¨à§‹,
à¦šà§‡à§Ÿà§‡ à¦¨à¦¾ à¦à¦¸à§‡ à¦¨à¦¾ à¦¬à§‡à¦¶à¦¿ à¦¬à§‡à¦¶à¦¿ à¦¬à§‡à¦¦à§‡à¥¤
à¦¬à¦¸à¦¨à§à¦¤ à¦¯à¦¬à§‡ à¦•à¦¾à¦²à¦¬à§ˆà¦¶à¦¾à¦–à§‡,
à¦¡à¦¾à¦²à¦¿ à¦ªà¦¾à¦›à§‡ à¦à¦°à¦¿à§Ÿà¦¾ à¦—à¦¾à¦à¦¥à¦¿à¦¤à§‡ à¦–à§à¦à¦œà§‡,
à¦«à§à¦² à¦¯à§‡ à¦ªà¦¾à¦›à§‡ à¦«à§à¦Ÿà§‡ à¦«à§à¦Ÿà§‡,
à¦¤à¦¾à¦°à¦¿ à¦¸à§à¦° à¦ªà¦¥à§‡ à¦ªà¦¾à¦¤à¦¾ à¦ªà¦¾à¦¤à¦¾ à¦¤à¦«à§à¦Ÿà§‡à¥¤
à¦¹à¦¾à¦¸à¦¿à¦®à§à¦–à§‡ à¦®à§‡à¦˜à§‡à¦° à¦•à§‹à¦£à§‡
à¦°à§Ÿà§‡à¦›à§‡ à¦šà¦¿à¦°à¦¦à¦¿à¦¨
à¦¬à§à¦¯à¦¾à¦ªà¦¿à§Ÿà¦¾ à¦®à¦§à§,
à¦ªà§à¦°à¦­à¦¾à¦¤à¦¸à¦‚à¦—à§€à¦¤
à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿à¦² à¦†à¦•à¦¾à¦¶-à¦•à§à¦®à¦²à¥¤
```

With a custom prompt: `"à¦¬à¦¿à¦¨à§Ÿ à¦­à¦¾à¦¬à¦›à§‡ à¦†à¦•à¦¾à¦¶ à¦•à§‡à¦¨ à¦¨à§€à¦²,"`

```
à¦¬à¦¿à¦¨à§Ÿ à¦­à¦¾à¦¬à¦›à§‡ à¦†à¦•à¦¾à¦¶ à¦•à§‡à¦¨ à¦¨à§€à¦²,
à¦­à¦¬à¦¿à¦·à§à¦¯à§ à¦®à¦¾à¦à§à¦šà¦¿à¦¤à§‡à¦° à¦¹à¦¾à¦¨à¦²;
à¦†à¦®à¦¾à¦¦à§‡à¦° à¦®à¦¨à§à¦¤à§à¦° à¦¦à§‡à¦–à¦›à¦¿ à¦¬à¦¿à¦¸à§à¦®à§Ÿà§‡à¦° à¦•à§à¦°à¦¨à§à¦¦à¦¨à§‡
à¦†à¦®à¦¾à¦•à§‡ à¦à¦¸à§‡à¦›à¦¿ à¦†à¦®à¦¾à¦° à¦¦à¦²à§‡, à¦•à§‡à¦¬à¦² à¦¸à¦¿à¦¹à¦¾à¦¸ à¦•à¦°à§‡à¥¤
à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸-à¦¬à§€à¦°à§‡ à¦•à¦²à§à¦ªà¦¨à¦¾...
```
âš ï¸ Note: Since the model was trained exclusively on Tagoreâ€™s poems, it may not mention modern names like â€œà¦¬à¦¿à¦¨à§Ÿâ€ unless they appear in the prompt. To guide the model, you can repeat your name in the prompt or use a leading token like "à¦¬à¦¿à¦¨à§Ÿ:".

> While writing prompt avoid using characters outside the known vocabulary to prevent unknown token issues. Unknown characters are currently mapped to space (' ').

---

## ğŸ§© Usage

### ğŸ”¹ Inference

You can generate text using:

```bash
python src/generate.py
```

You can adjust **max_new_tokens** and **temperature** in `src/generate.py` to control generation length and creativity.

---

### ğŸ§¾ How to Train

```bash
python src/train.py
```

Tweak hyperparameters in `src/train.py` as needed.

---

## ğŸ“œ Acknowledgements

- Inspired by Andrej Karpathyâ€™s nanoGPT implementation.

- Trained and fine-tuned on Rabindranath Tagoreâ€™s Bengali poem corpus.

- Dataset: Public domain Bengali poems by Rabindranath Tagore, available on Kaggle.  

- Developed and trained by **Binoy Bhushan Barman Dipu** as part of learning and experimentation with GPT architectures.

---

## ğŸ’¡ Future Work

- Implement BPE or WordPiece tokenization for better semantic coherence.

- Experiment with larger context lengths (512+) and deeper models.

- Add Bangla rhyme and meter control for stylistic conditioning.

- Integrate UI-based poem generator using Streamlit or Flask.



> â€œKobiGPT is not just a model â€” itâ€™s a poetic mirror where Tagoreâ€™s spirit of words finds a neural echo.â€